{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf46edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File data_window.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_selection, feature_selection, linear_model, metrics\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImport data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_window.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m X\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m X2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_hdf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_window3.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/pytables.py:427\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m     exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists:\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_buf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    429\u001b[0m store \u001b[38;5;241m=\u001b[39m HDFStore(path_or_buf, mode\u001b[38;5;241m=\u001b[39mmode, errors\u001b[38;5;241m=\u001b[39merrors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# so delegate to the iterator\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File data_window.h5 does not exist"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: Antoine DELPLACE\n",
    "# Last update: 17/01/2020\n",
    "\"\"\"\n",
    "Perform a statistic analysis of the Neural network classifier.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "data_window_botnetx.h5         : extracted data from preprocessing1.py\n",
    "data_window3_botnetx.h5        : extracted data from preprocessing2.py\n",
    "data_window_botnetx_labels.npy : label numpy array from preprocessing1.py\n",
    "nb_prediction                  : number of predictions to perform\n",
    "\n",
    "Return\n",
    "----------\n",
    "Print train and test mean accuracy, precison, recall, f1\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, Dense, Dropout, Flatten, Lambda, MaxPool2D, Conv2DTranspose, UpSampling2D, Concatenate, Add\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn import model_selection, feature_selection, utils, ensemble, linear_model, metrics\n",
    "\n",
    "print(\"Import data\")\n",
    "\n",
    "X = pd.read_hdf('data_window_botnet7.h5', key='data')\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X2 = pd.read_hdf('data_window3_botnet7.h5', key='data')\n",
    "X2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = X.join(X2)\n",
    "\n",
    "X.drop('window_id', axis=1, inplace=True)\n",
    "\n",
    "y = X['Label_<lambda>']\n",
    "X.drop('Label_<lambda>', axis=1, inplace=True)\n",
    "\n",
    "labels = np.load(\"data_window_botnet7_labels.npy\")\n",
    "\n",
    "print(X.columns.values)\n",
    "print(labels)\n",
    "print(np.where(labels == 'flow=From-Botne')[0][0])\n",
    "\n",
    "y_bin6 = y==np.where(labels == 'flow=From-Botne')[0][0]\n",
    "print(\"y\", np.unique(y, return_counts=True))\n",
    "\n",
    "## NN\n",
    "filename_weights = \"model.h5\"\n",
    "\n",
    "def fprecision(y_true, y_pred):\t\n",
    "    \"\"\"Precision metric.\t\n",
    "    Only computes a batch-wise average of precision. Computes the precision, a\n",
    "    metric for multi-label classification of how many selected items are\n",
    "    relevant.\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\t\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\t\n",
    "    return precision\n",
    "\n",
    "def frecall(y_true, y_pred):\t\n",
    "    \"\"\"Recall metric.\t\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\t\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\t\n",
    "    return recall\n",
    "\n",
    "def ff1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F1 Score\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\n",
    "    p = fprecision(y_true, y_pred)\n",
    "    r = frecall(y_true, y_pred)\n",
    "    return (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "def get_model(inputs, dropout=0.5, batchnorm=True):\n",
    "    x = Dense(256, input_shape=(22,))(inputs)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    x = Dense(128, input_shape=(256,))(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    x = Dense(1, input_shape=(128,))(x)\n",
    "    outputs = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "nb_prediction = 50\n",
    "np.random.seed(seed=123456)\n",
    "tab_seed = np.random.randint(0, 1000000000, nb_prediction)\n",
    "print(tab_seed)\n",
    "\n",
    "tab_train_precision = np.array([0.]*nb_prediction)\n",
    "tab_train_recall = np.array([0.]*nb_prediction)\n",
    "tab_train_fbeta_score = np.array([0.]*nb_prediction)\n",
    "\n",
    "tab_test_precision = np.array([0.]*nb_prediction)\n",
    "tab_test_recall = np.array([0.]*nb_prediction)\n",
    "tab_test_fbeta_score = np.array([0.]*nb_prediction)\n",
    "\n",
    "for i in range(0, nb_prediction):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y_bin6, test_size=0.33, random_state=tab_seed[i])\n",
    "\n",
    "    print(i)\n",
    "    print(\"y_train\", np.unique(y_train, return_counts=True))\n",
    "    print(\"y_test\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "    inputs = Input((22,), name='input')\n",
    "    model = get_model(inputs, dropout=0, batchnorm=1)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filename_weights, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3), loss=[\"binary_crossentropy\"], metrics=[fprecision, frecall, ff1_score])\n",
    "    #model.summary()\n",
    "\n",
    "    tps = time.time()\n",
    "    results = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_split=0.15, shuffle=True, class_weight=None, verbose=0, callbacks=callbacks)\n",
    "    print(\"Execution time = \", time.time()-tps)\n",
    "\n",
    "    model.load_weights(filename_weights)\n",
    "\n",
    "    y_pred_train = model.predict(X_train, batch_size=32, verbose=0)\n",
    "    y_pred_train_bin = (y_pred_train > 0.5).astype(np.uint8)\n",
    "    precision, recall, fbeta_score, support = metrics.precision_recall_fscore_support(y_train, y_pred_train_bin)\n",
    "    tab_train_precision[i] = precision[1]\n",
    "    tab_train_recall[i] = recall[1]\n",
    "    tab_train_fbeta_score[i] = fbeta_score[1]\n",
    "\n",
    "    y_pred_test = model.predict(X_test, batch_size=32, verbose=0)\n",
    "    y_pred_test_bin = (y_pred_test > 0.5).astype(np.uint8)\n",
    "    precision, recall, fbeta_score, support = metrics.precision_recall_fscore_support(y_test, y_pred_test_bin)\n",
    "    tab_test_precision[i] = precision[1]\n",
    "    tab_test_recall[i] = recall[1]\n",
    "    tab_test_fbeta_score[i] = fbeta_score[1]\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"precision = \", tab_train_precision.mean(), tab_train_precision.std(), tab_train_precision)\n",
    "print(\"recall = \", tab_train_recall.mean(), tab_train_recall.std(), tab_train_recall)\n",
    "print(\"fbeta_score = \", tab_train_fbeta_score.mean(), tab_train_fbeta_score.std(), tab_train_fbeta_score)\n",
    "\n",
    "print(\"Test\")\n",
    "print(\"precision = \", tab_test_precision.mean(), tab_test_precision.std(), tab_test_precision)\n",
    "print(\"recall = \", tab_test_recall.mean(), tab_test_recall.std(), tab_test_recall)\n",
    "print(\"fbeta_score = \", tab_test_fbeta_score.mean(), tab_test_fbeta_score.std(), tab_test_fbeta_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090809c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78566cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
